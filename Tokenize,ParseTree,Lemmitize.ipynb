{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a754548",
   "metadata": {},
   "source": [
    "# Tokenizer,Named Entity,Parse Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833be697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wo', \"n't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"Won't\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83ce60a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'cant', \"'\", 't', 'cum', 'h', \"'\", 'ome']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WordPunktTokenizer - Seperate words and punctuation as Tokens\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "wpt = WordPunctTokenizer()\n",
    "wpt.tokenize(\"Hello cant't cum h'ome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b11716b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vaadi en tamil selvi.', 'i take u shopping']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing sentences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "st(\"\"\"Vaadi en tamil selvi.\n",
    "   i take u shopping\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37960786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming\n",
    "from nltk.stem import PorterStemmer as ps\n",
    "stmm=ps()\n",
    "stmm.stem(\"doing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1ba228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RegExpStemmer\n",
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "Reg_stemmer = RegexpStemmer(\"s\")\n",
    "Reg_stemmer.stem(\"believes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e647bcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lmt = WordNetLemmatizer()\n",
    "lmt.lemmatize(\"believes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d23790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(s (np (np (n fish)) (np (n people))) (vp (v fish) (np (n tanks))))\n"
     ]
    }
   ],
   "source": [
    "#NLTK-TREE\n",
    "from nltk.tree import *\n",
    "st1 = Tree('np',[Tree('n',['fish'])])\n",
    "st2 = Tree('np',[Tree('n',['people'])])\n",
    "st3 = Tree('np',[Tree('n',['tanks'])])\n",
    "np1 = Tree('np',[st1,st2])\n",
    "vp1 = Tree('vp',[Tree('v',['fish']),st3])\n",
    "s = Tree('s',[np1,vp1])\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d7b54d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 s                \n",
      "       __________|________         \n",
      "      np                  vp      \n",
      "  ____|____           ____|____    \n",
      " np        np        |         np \n",
      " |         |         |         |   \n",
      " n         n         v         n  \n",
      " |         |         |         |   \n",
      "fish     people     fish     tanks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trees=\"(s (np (np (n fish)) (np (n people))) (vp (v fish) (np (n tanks))))\"\n",
    "s=Tree.fromstring(trees)\n",
    "s.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e948061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (DT the) (N child))\n",
      "  (VP\n",
      "    (V ate)\n",
      "    (NP\n",
      "      (NP (DT the) (N cake))\n",
      "      (PP (Pre with) (NP (DT a) (N fork)))))) (p=0.000373248)\n"
     ]
    }
   ],
   "source": [
    "#parsing a tree using set of rules\n",
    "import nltk\n",
    "from nltk.tree import *\n",
    "rules=nltk.PCFG.fromstring(\"\"\"\n",
    "S -> NP VP [1.0]\n",
    "PP -> Pre NP [1.0]\n",
    "NP -> NP PP [0.4] \n",
    "NP -> DT N [0.6]\n",
    "VP -> V NP [1.0]\n",
    "DT -> \"a\" [0.4]\n",
    "DT -> \"the\" [0.6]\n",
    "N -> \"cake\" [0.3]\n",
    "N -> \"child\" [0.5]\n",
    "N -> \"fork\" [0.2]\n",
    "Pre -> \"with\" [1.0]\n",
    "V -> \"ate\" [1.0]\n",
    "\n",
    "\"\"\") # Create grammar rules for the tree\n",
    "trees=[]\n",
    "sentence=[\"the\",\"child\",\"ate\",\"the\",\"cake\",\"with\",\"a\",\"fork\"] #Initialize the Sentence\n",
    "parser=nltk.ViterbiParser(rules) #assign rules to the class\n",
    "for tree in parser.parse(sentence): # using rules create parse trees\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d32128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the SentenceUS is a part Of United Nations\n",
      "(GSP US/NNP)\n",
      "('is', 'VBZ')\n",
      "('a', 'DT')\n",
      "('part', 'NN')\n",
      "('Of', 'IN')\n",
      "(ORGANIZATION United/NNP Nations/NNP)\n"
     ]
    }
   ],
   "source": [
    "#Named Entity Recognition\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(input(\"Enter the Sentence\"))\n",
    "\n",
    "tagged=nltk.pos_tag(words)\n",
    "\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e1b20",
   "metadata": {},
   "source": [
    "# hypothesis testing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfad4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
